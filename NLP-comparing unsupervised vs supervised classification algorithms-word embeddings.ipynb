{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Preface\n",
    "This notebook explores the same four categories from the 20 Newsgroup data set. The same supervised and unsupervised algorithms are run below however I dropped LDA from this notebook as the results were continuously way worse than baseline. One difference versus the other notebook is that the supervised classifier does not arrive at its predictions via any gridsearch optimization. \n",
    "\n",
    "This was more of a curiosity than anything else. Mikolov's models have been hugely influential globally and I definitely respect his leaning towards parsimony whenever possible. It is interesting to me how far NLP has come since his seminal papers on Word2vec, Doc2vec and Fasttext. It is also interesting that more simple count based models like TF and TFIDF still do ok on simple classification tasks.\n",
    "\n",
    "The word embeddings are derived from Gensim's Doc2vec version of the original Mikolov model. I am using the DBOW model for the paragraph vectors along with the skipgram model for the word vectors. Both sets of vectors are concatenated during training. I avoided using hierarchical softmax and favoured negative sampling(20 random noise words). Model was trained over just five epochs, alpha was set at 0.001. \n",
    "\n",
    "\n",
    "Results are below\n",
    "\n",
    "\n",
    "#### References:\n",
    "Blei, Ng, Jorden(2003): Latent Dirichlet Allocation,\n",
    "https://ai.stanford.edu/~ang/papers/nips01-lda.pdf\n",
    "\n",
    "Gensim, PyData Berlin (2016): \n",
    "https://github.com/RaRe-Technologies/movie-plots-by-genre\n",
    " \n",
    "Greene, O’Callaghan, Cunningham(2014): 'How Many Topics? Stability Analysis for Topic Models' https://arxiv.org/abs/1404.4606\n",
    "  \n",
    "Li(2018): Multi-Class Text Classification with Doc2Vec & Logistic Regression\n",
    "https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4\n",
    "\n",
    "Mikolov, Lee(2014): Distributed Representations of Sentences and Documents.\n",
    "https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "\n",
    "#### Aug 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import numpy as np \n",
    "import re, nltk, spacy, gensim\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline \n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    " \n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import multiprocessing\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify topic categories used in analysis,\n",
    "cats = ['rec.sport.baseball', 'rec.sport.hockey','talk.politics.guns', 'talk.politics.mideast']  \n",
    "target_names = ['Baseball', 'Hockey', 'Guns','Middle East']\n",
    "\n",
    "#Scaler to convert any negative embedding values to positive values for model \n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "\n",
    "\n",
    "#Get the data\n",
    "def get_data():\n",
    "   global df\n",
    "   data_tr = fetch_20newsgroups(subset='train', categories=cats,shuffle=True, random_state=1,\n",
    "                             remove=('headers','footers', 'quotes'))  \n",
    "   data_te = fetch_20newsgroups(subset='test', categories=cats,shuffle=True, random_state=1,\n",
    "                             remove=('headers','footers', 'quotes'))\n",
    "   df_train= pd.DataFrame(data_tr.data, columns=['comment'])\n",
    "   df_train['labels']=data_tr.target \n",
    "   df_test= pd.DataFrame(data_te.data, columns=['comment'])  \n",
    "   df_test['labels']=data_te.target\n",
    "   df= pd.concat([df_train,df_test])\n",
    "\n",
    "#Pre processing cleaning\n",
    "def cleanText(text):   \n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "\n",
    "#Tokenize\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    " \n",
    "#Prep training and test data for models \n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, embeddings = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, embeddings\n",
    "\n",
    "#Train doc2vec model with 500 features    \n",
    "def train_model():\n",
    "    global model_dbow     \n",
    "    cores = multiprocessing.cpu_count()\n",
    "    #Using 400 dimension feature space, a PV-DBOW model, word vectors are also trained using the skipgram model \n",
    "    #and concatenated with paragraph vectors. Word window is set at five words. Noise reduction during training \n",
    "    #is via negative sampling with twenty random words feed to the model at a time.    \n",
    "    model_dbow = Doc2Vec(dm=0,dm_concat=1,vector_size=400, dbow_words=1,window=5, negative=20, hs=0,workers=4)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])  \n",
    "    #train for just five epochs with alpha set at 0.001\n",
    "    for epoch in range(5):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.001\n",
    "        model_dbow.min_alpha = model_dbow.alpha     \n",
    "\n",
    "#Generate embeddings        \n",
    "def vec_for_learning(model, tagged_docs): \n",
    "    sents = tagged_docs.values\n",
    "    targets, embeddings = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, embeddings\n",
    "\n",
    "#Generate tagged docs\n",
    "def run_tags():\n",
    "    global train_tagged,test_tagged\n",
    "    train, test = train_test_split(df, test_size=0.6, shuffle=False,random_state=42)\n",
    "    train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['comment']), tags=[r.labels]), axis=1)\n",
    "    test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['comment']), tags=[r.labels]), axis=1)\n",
    "\n",
    "#Call the Doc2Vec model generate word embedding vectors for models to use as predictors\n",
    "def call_dbow():\n",
    "    global y_train, X_train,y_test, X_test\n",
    "    run_tags()\n",
    "    train_model()\n",
    "    y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "                   \n",
    "#Run supervised classifier\n",
    "def run_supervised_classifier(): \n",
    "    clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5000, tol=1e-6)\n",
    "    X_train_sc= scaler.fit_transform(X_train)\n",
    "    X_test_s=scaler.transform(X_test)\n",
    "    clf.fit(X_train_sc, y_train)\n",
    "    y_pred=clf.predict(X_test_s)\n",
    "    print('')\n",
    "    print('Supervised classifier metrics')\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))   \n",
    "                    \n",
    "#Run NMF model     \n",
    "def run_nnmf():\n",
    "    nnmf = NMF(n_components=4, random_state=1, init='nndsvd', max_iter=500, alpha=0.0,l1_ratio=0.0)\n",
    "    X_train_sc= scaler.fit_transform(X_train)\n",
    "    X_test_s=scaler.transform(X_test)\n",
    "    X_test_sc=np.absolute(X_test_s)\n",
    "    nnmf_train= nnmf.fit_transform(X_train_sc)\n",
    "    nnmf_test = nnmf.transform(X_test_sc)\n",
    "    nnmf_pred= nnmf_test.argmax(axis=1)    \n",
    "    print('')\n",
    "    print(\"NMF classification metrics\")\n",
    "    print(classification_report(y_test, nnmf_pred, target_names=target_names))\n",
    "                  \n",
    "#Main function to compare each of the models        \n",
    "def compare_models():\n",
    "    get_data()\n",
    "    df['comment'] = df['comment'].apply(cleanText)    \n",
    "    run_tags() \n",
    "    call_dbow()\n",
    "    run_supervised_classifier()\n",
    "    run_nnmf() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1537/1537 [00:00<00:00, 711471.72it/s]\n",
      "100%|██████████| 1537/1537 [00:00<00:00, 299273.26it/s]\n",
      "100%|██████████| 1537/1537 [00:00<00:00, 886868.24it/s]\n",
      "100%|██████████| 1537/1537 [00:00<00:00, 984671.64it/s]\n",
      "100%|██████████| 1537/1537 [00:00<00:00, 899866.73it/s]\n",
      "100%|██████████| 1537/1537 [00:00<00:00, 1131983.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Supervised classifier metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Baseball       0.59      0.95      0.73       604\n",
      "      Hockey       0.92      0.73      0.81       603\n",
      "        Guns       0.94      0.65      0.77       525\n",
      " Middle East       0.92      0.79      0.85       574\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      2306\n",
      "   macro avg       0.84      0.78      0.79      2306\n",
      "weighted avg       0.84      0.78      0.79      2306\n",
      "\n",
      "\n",
      "NMF classification metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Baseball       0.40      0.87      0.55       604\n",
      "      Hockey       0.03      0.02      0.02       603\n",
      "        Guns       0.94      0.54      0.68       525\n",
      " Middle East       0.55      0.32      0.41       574\n",
      "\n",
      "   micro avg       0.43      0.43      0.43      2306\n",
      "   macro avg       0.48      0.44      0.42      2306\n",
      "weighted avg       0.47      0.43      0.41      2306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Call main function\n",
    "compare_models()     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
